{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8: Nearest Neighbors Regression üèò"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Mahoto Sasaki\n",
    "\n",
    "Student ID: 467695\n",
    "\n",
    "Collaborators:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "In this homework, we will be exploring a more realistic application of similarity-based leanring. It might be helpful to review **Lab 8 (Feature Transformation and Similarity-based Prediction with k-NN)** first. Most of the things we ask you to do in this homework are explained in the lab. In general, you should feel free to import any package that we have previously used in class. Ensure that all plots have the necessary components that a plot should have (e.g. axes labels, a title, a legend).\n",
    "\n",
    "Furthermore, in addition to recording your collaborators on this homework, please also remember to cite/indicate all external sources used when finishing this assignment. This includes peers, TAs, and links to online sources. Note that these citations will not free you from your obligation to submit your _own_ code and write-ups, however, they will be taken into account during the grading and regrading process.\n",
    "\n",
    "### Submission instructions\n",
    "* Submit this python notebook including your answers in the code cells as homework submission.\n",
    "* **Feel free to add as many cells as you need to** ‚Äî just make sure you don't change what we gave you. \n",
    "* **Does it spark joy?** Note that you will be partially graded on the presentation (_cleanliness, clarity, comments_) of your notebook so make sure you [Marie Kondo](https://lifehacker.com/marie-kondo-is-not-a-verb-1833373654) your notebook before submitting it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using `sklearn` for $k$-Nearest Neighbors\n",
    "\n",
    "In Lab 8, we got familiar with $k$-nearest neighbors ($k$-NN) by implementing the algorithm. If you are still not comfortable with how the algorithm works, then we suggest that you review your work from the lab. We will proceed here under this assumption.\n",
    "\n",
    "In this section, we will explore how to use the [$k$-NN _regression_ model supplied by `sklearn`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor). You can find the [$k$-NN _classification_ model here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Some Data\n",
    "\n",
    "We'll need to start by getting some data ‚Äî what is data science without data? For this assignment, we will be revisiting another old friend: the Boston Housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're here, let's review what this dataset is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.1\n",
    "\n",
    "**Write-up!** How many examples are in the dataset? How many features does it have? What are the features? What is the target variable that we would like to estimate? What kind of machine learning problem is this?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "The dataset has 506 examples and 13 features. The features are:\n",
    "        - CRIM     per capita crime rate by town\n",
    "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "        - INDUS    proportion of non-retail business acres per town\n",
    "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "        - NOX      nitric oxides concentration (parts per 10 million)\n",
    "        - RM       average number of rooms per dwelling\n",
    "        - AGE      proportion of owner-occupied units built prior to 1940\n",
    "        - DIS      weighted distances to five Boston employment centres\n",
    "        - RAD      index of accessibility to radial highways\n",
    "        - TAX      full-value property-tax rate per $10,000\n",
    "        - PTRATIO  pupil-teacher ratio by town\n",
    "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "        - LSTAT    % lower status of the population\n",
    "        - MEDV     Median value of owner-occupied homes in $1000's\n",
    "The target variable is the Boston house price. \n",
    "This is a supervised regression machine learning problem to find the features that affect the house price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Data\n",
    "\n",
    "In the lab, we also looked at data scaling and transformations. Here we'll demonstrate how to use `sklearn` to help us with this. Let's call this **approach 1**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# new train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "\n",
    "# compute the mean and standard deviation on a training set \n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "# apply the transforamtion to both the trainnig and the test set\n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative and much quicker way of scaling the the data is the following (let's call this **approach 2**): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.2\n",
    "\n",
    "**Write-up** What types of scaling does `StandardScaler()` and `scale` perform? `Hint` Use the [`?` operator](https://ipython.readthedocs.io/en/stable/interactive/python-ipython-diff.html#accessing-help). Which of the two proceedures is a more appropriate preprocessing step for supervised machine learning and _why_? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "StandardScaler() standardizes an individual feature by removing the mean and scaling to unit variance. scale() standardizes a dataset along any axis by centering to the mean and using component wise scale to unit variance. StandardScaler() is a better procedure for the preprocessing step because it computes the mean and standard deviation on a training set in order to be able to reapply the same transformation on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing.scale?\n",
    "#preprocessing.StandardScaler?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking Into the Model\n",
    "\n",
    "Now that we have some data to play with, let's try building a $k$-NN regression model. The model provided by `sklearn` shares the a similar interface as the other models that we have looked at previously (esp. $k$-means)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.3\n",
    "\n",
    "Use the [`?` operator](https://ipython.readthedocs.io/en/stable/interactive/python-ipython-diff.html#accessing-help) provided by IPython to explore `model` and it's interface.\n",
    "\n",
    "**Try this!** In the cell below, complete the following:\n",
    "1. create and fit a new `KNeighborsRegressor` model with 5 neighbors\n",
    "2. make some predictions using the model on your testing data\n",
    "3. evaluate the performance of the model by computing $R^2$ and storing it in `r_squared`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7153853569196222"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# your code here\n",
    "new_model = KNeighborsRegressor(n_neighbors=5)\n",
    "new_model.fit(X_train, y_train)\n",
    "\n",
    "r_squared = new_model.score(X = X_test, y = y_test)\n",
    "\n",
    "assert np.isclose(r_squared, 0.71538, rtol=1e-4), 'You should see this R^2 value'\n",
    "\n",
    "r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        KNeighborsRegressor\n",
       "\u001b[0;31mString form:\u001b[0m\n",
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                    weights='uniform')\n",
       "\u001b[0;31mFile:\u001b[0m        /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neighbors/_regression.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "Regression based on k-nearest neighbors.\n",
       "\n",
       "The target is predicted by local interpolation of the targets\n",
       "associated of the nearest neighbors in the training set.\n",
       "\n",
       "Read more in the :ref:`User Guide <regression>`.\n",
       "\n",
       ".. versionadded:: 0.9\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_neighbors : int, optional (default = 5)\n",
       "    Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
       "\n",
       "weights : str or callable\n",
       "    weight function used in prediction.  Possible values:\n",
       "\n",
       "    - 'uniform' : uniform weights.  All points in each neighborhood\n",
       "      are weighted equally.\n",
       "    - 'distance' : weight points by the inverse of their distance.\n",
       "      in this case, closer neighbors of a query point will have a\n",
       "      greater influence than neighbors which are further away.\n",
       "    - [callable] : a user-defined function which accepts an\n",
       "      array of distances, and returns an array of the same shape\n",
       "      containing the weights.\n",
       "\n",
       "    Uniform weights are used by default.\n",
       "\n",
       "algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n",
       "    Algorithm used to compute the nearest neighbors:\n",
       "\n",
       "    - 'ball_tree' will use :class:`BallTree`\n",
       "    - 'kd_tree' will use :class:`KDTree`\n",
       "    - 'brute' will use a brute-force search.\n",
       "    - 'auto' will attempt to decide the most appropriate algorithm\n",
       "      based on the values passed to :meth:`fit` method.\n",
       "\n",
       "    Note: fitting on sparse input will override the setting of\n",
       "    this parameter, using brute force.\n",
       "\n",
       "leaf_size : int, optional (default = 30)\n",
       "    Leaf size passed to BallTree or KDTree.  This can affect the\n",
       "    speed of the construction and query, as well as the memory\n",
       "    required to store the tree.  The optimal value depends on the\n",
       "    nature of the problem.\n",
       "\n",
       "p : integer, optional (default = 2)\n",
       "    Power parameter for the Minkowski metric. When p = 1, this is\n",
       "    equivalent to using manhattan_distance (l1), and euclidean_distance\n",
       "    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
       "\n",
       "metric : string or callable, default 'minkowski'\n",
       "    the distance metric to use for the tree.  The default metric is\n",
       "    minkowski, and with p=2 is equivalent to the standard Euclidean\n",
       "    metric. See the documentation of the DistanceMetric class for a\n",
       "    list of available metrics.\n",
       "    If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
       "    must be square during fit. X may be a :term:`Glossary <sparse graph>`,\n",
       "    in which case only \"nonzero\" elements may be considered neighbors.\n",
       "\n",
       "metric_params : dict, optional (default = None)\n",
       "    Additional keyword arguments for the metric function.\n",
       "\n",
       "n_jobs : int or None, optional (default=None)\n",
       "    The number of parallel jobs to run for neighbors search.\n",
       "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
       "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
       "    for more details.\n",
       "    Doesn't affect :meth:`fit` method.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "effective_metric_ : string or callable\n",
       "    The distance metric to use. It will be same as the `metric` parameter\n",
       "    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
       "    'minkowski' and `p` parameter set to 2.\n",
       "\n",
       "effective_metric_params_ : dict\n",
       "    Additional keyword arguments for the metric function. For most metrics\n",
       "    will be same with `metric_params` parameter, but may also contain the\n",
       "    `p` parameter value if the `effective_metric_` attribute is set to\n",
       "    'minkowski'.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> X = [[0], [1], [2], [3]]\n",
       ">>> y = [0, 0, 1, 1]\n",
       ">>> from sklearn.neighbors import KNeighborsRegressor\n",
       ">>> neigh = KNeighborsRegressor(n_neighbors=2)\n",
       ">>> neigh.fit(X, y)\n",
       "KNeighborsRegressor(...)\n",
       ">>> print(neigh.predict([[1.5]]))\n",
       "[0.5]\n",
       "\n",
       "See also\n",
       "--------\n",
       "NearestNeighbors\n",
       "RadiusNeighborsRegressor\n",
       "KNeighborsClassifier\n",
       "RadiusNeighborsClassifier\n",
       "\n",
       "Notes\n",
       "-----\n",
       "See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
       "for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
       "\n",
       ".. warning::\n",
       "\n",
       "   Regarding the Nearest Neighbors algorithms, if it is found that two\n",
       "   neighbors, neighbor `k+1` and `k`, have identical distances but\n",
       "   different labels, the results will depend on the ordering of the\n",
       "   training data.\n",
       "\n",
       "https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Problem 1.4\n",
    "\n",
    "**Write-up!** What was the $R^2$ value for your $k$-NN model using five neighbors? What does $R^2$ tell you about a model? What does this score tell you about your model?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "The R^2 value for the k-NN mode was 0.71538. This tells us that the model produced fairly accurate predictions using the testing data and it was close to the ground truth values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, let's move on to some more interesting things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choosing $k$ with Cross Validation\n",
    "\n",
    "In order to test whether the `kNN` algorithm (or any other machine learning algorithm) performs how we want it to and accurately makes predictions, we must compare the known label of all datapoints to the predicted label of those same datapoints. So far we have seen this in the forms of model evaluation and validation in model selection. \n",
    "\n",
    "In model evaluation we partitioned our original dataset into two parts: a training set and a testing set. As we have seen earlier in the course, the testing set is a smaller percentage of the total dataset than the training set.\n",
    "\n",
    "Later on, in model selection, we explored why it was important to have yet another set of data partitioned out for usage as a validation set, which we could use to experiment with a model's hyperparameters. The validation set allowed use to \"evaluate\" our model's performance with various settings of it's parameters while maintaining a completely untouched dataset for out final evaluation.\n",
    "\n",
    "We can extend this idea once again to improve our estimates of model performance through **cross validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `kFolds` method\n",
    "\n",
    "One version of cross validation partitions the dataset into `k` partitions, or folds. We use `k-1` folds to train the model, then the one fold we left out to test the model. We iterate this process `k` times, leaving out a different fold each time, so that we have an accuracy score for each one of the `k` different partitions. We can then take the average of all of these accuracies to calculate a more wholistic accuracy representation of the algorithm. In the example below, `k = 5`; there are 5 partitions. Each partition is used once as a test partition while the other 4 are used for training purposes. The idea for $k$-fold cross validation is based on the realization that we can get a better picture of our model's performance by feeding it many combinations of our data.\n",
    "\n",
    "![](utility/pics/kFold.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `KFold` function to partition our dataset into `k` partitions. While the `KFold` function does not split the dataset itself, it provides the indices by which to split the dataset.\n",
    "\n",
    "Below, we split an arbitrary array of length 10 into 5 folds: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 0: Train indices: [0 1 3 4 5 6 8 9]. Test indices: [2 7]\n",
      "For iteration 1: Train indices: [0 1 2 5 6 7 8 9]. Test indices: [3 4]\n",
      "For iteration 2: Train indices: [0 1 2 3 4 5 7 9]. Test indices: [6 8]\n",
      "For iteration 3: Train indices: [0 1 2 3 4 6 7 8]. Test indices: [5 9]\n",
      "For iteration 4: Train indices: [2 3 4 5 6 7 8 9]. Test indices: [0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "dummy = np.arange(10) # example data\n",
    "\n",
    "# initialize KFolds\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# iterating over k different splits of dummy\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(dummy)):\n",
    "    print(f'For iteration {fold}: Train indices: {train_idx}. Test indices: {test_idx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how each testing datapoint appears once, ensuring that all datapoints have had a chance to be be tested against the model trained with the rest of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.1\n",
    "\n",
    "Now, let's try using the `KFold` operation on the full Boston Housing dataset, building and fitting new $k$-NN models with each fold, and averaging the scores of each model.\n",
    "\n",
    "**Try this!** Complete the `knn_kfolds` function so that it performs `n_folds`-fold cross validation of $k$-NN models on `X` using `n_neighbors` and returns the average $R^2$ value of the models in `avg_score`.\n",
    "\n",
    "* Make sure to scale your training and test sets appropriately (√† la the [Scaling Data](#Scaling-Data) section).\n",
    "* Ensure that you make and fit a new model for each fold.\n",
    "* Also, please make sure that you set `random_state` appropriately in your initialization of `KFold`.\n",
    "\n",
    "`Hint` Refer to the previous example of how to use `KFold` and your work in [Problem 1.2](#Problem-1.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_kfolds(X, y, n_folds, n_neighbors, random_state=None):\n",
    "    '''Computes'''\n",
    "\n",
    "    # your code here\n",
    "    totalScore = 0;\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):        \n",
    "        X_train1, X_test1 = X[train_idx], X[test_idx]\n",
    "        y_train1, y_test1 = y[train_idx], y[test_idx]\n",
    "        \n",
    "        scaler = preprocessing.StandardScaler().fit(X_train1)\n",
    "        X_train1 = scaler.transform(X_train1)  \n",
    "        X_test1 = scaler.transform(X_test1) \n",
    "        \n",
    "        model = KNeighborsRegressor(n_neighbors)\n",
    "        model.fit(X_train1, y_train1)\n",
    "        totalScore+= model.score(X = X_test1, y = y_test1)\n",
    "    avg_score = totalScore / n_folds\n",
    "    \n",
    "    assert np.isscalar(avg_score), 'The average score should be a single number'\n",
    "    assert 0 <= avg_score and avg_score <= 1, 'The average score should be between 0 and 1'\n",
    "    \n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing $k$\n",
    "\n",
    "We can use cross validation as a substitute for the model selection algorithm that we've used in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2\n",
    "\n",
    "In this problem, we will use cross validation and our `knn_kfolds` function to help us pick the right $k$ to use for our Boston Housing predictions.\n",
    "\n",
    "**Try this!** In the following cell, use 10-fold cross validation to evaluate the performance of $k$-NN on $X_{\\text{scaled}}$ and $y$ from the Boston Housing dataset and provide a plot of the cross validation average $R^2$ values for $k$ values from 1 to 20 (inclusive). Use a random state of 12 for your analysis. Ensure that your plot has the appropriate components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(20):\n",
    "    avg_score = knn_kfolds(X_scaled, y, 10, i+1, 12)\n",
    "    plt.scatter(i+1, avg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3\n",
    "\n",
    "**Write-up!** Based on your plot from [Problem 2.2](#Problem-2.2), which $k$ value would you pick for your final model? Explain why."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "I would pick k=3 because that is where the R^2 value is maximized. Having the largest R^2 means that it has the least error out of all the other models of k in terms of how well it predicts the target value compared to the ground truth values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Selection with Cross Validation\n",
    "\n",
    "As mentioned before, we can use cross validation to get a more thorough evaluation of model performance. This means that we can use if for model selection by substituting it for the validation set process that we have used in the past.\n",
    "\n",
    "In this section, we will compare our $k$-NN regression model with a linear regression model that we used back in Lab 4 when we last looked at the Boston Housing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.1\n",
    "\n",
    "**Try this!** In the following cell, report the cross validation score (average $R^2$) of a $k$-NN model with the $k$ you selected in [Problem 2.3](#Problem-2.3) on `X_scaled`. Use a random state of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8080891806722349"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "knn_kfolds(X_scaled, y, n_folds=10, n_neighbors=3, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.2\n",
    "\n",
    "Now let's do 10-fold cross validation on a linear regression model on `X` without scaling.\n",
    "\n",
    "**Write-up** Why should shouldn't we use scaling here? What will happen if we do?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "We should not use scaling here because the weights of linear regression will be different if you scale. This will alter the data becasue we are working with different coefficients for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Perform 10-fold cross validation for linear regression on `X` and report the average $R^2$ value across all of the folds. Make sure to create and fit new models for each fold of the process. Use a random state of 5. `Hint` Refer to your work in [Problem 2.1](#Problem-2.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7421121752607969\n",
      "0.738006563361719\n",
      "0.7608511046800274\n",
      "0.7409344708934305\n",
      "0.7357498294949991\n",
      "0.742610609360596\n",
      "0.7413763147424426\n",
      "0.7450119040998939\n",
      "0.7469579457511123\n",
      "0.7277865054815109\n",
      "\n",
      "0.7421397423126528\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "from sklearn.linear_model import LinearRegression\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=5)\n",
    "totalScore = 0\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X)):        \n",
    "    X_train1, X_test1 = X[train_idx], X[test_idx]\n",
    "    y_train1, y_test1 = y[train_idx], y[test_idx]\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train1, y_train1)\n",
    "    print(model.score(X = X_train1, y = y_train1))\n",
    "    totalScore+= model.score(X = X_train1, y = y_train1)\n",
    "avg_score = totalScore / 10\n",
    "print()\n",
    "print(avg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.3\n",
    "\n",
    "**Write-up!** What were the $R^2$ values for each of the models? Which model would you prefer? Why?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "The R^2 values for the linear model was 0.7421397423126528 and for the KNeighborsRegressor was 0.8080891806722349. I prefer the KNeighborsRegressor because it had the highest R^2 which indicates that it has the least error. Therefore, the KNeighborsRegressor is the more accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.4\n",
    "\n",
    "**Write-up!** Describe your next steps as a data scientist now that you have decided which model to use."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "The next steps I could take are to see which features have influence on the housing prices by narrowing down the variables and seeing their influence on the target variable. I could identify what determines the Boston housing prices. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
